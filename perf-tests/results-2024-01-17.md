# Load Test Results - January 17, 2024

## Test Configuration

- **Tool:** k6 v0.48.0
- **Duration:** 3 minutes
- **Target Load:** 30 concurrent VUs
- **Test Environment:** Local laptop (16GB RAM), Docker Compose
- **Database:** PostgreSQL with 10M test events (not 100B, but enough to test)

## Results Summary

```
     ✓ status is 200
     ✓ has events

     checks.........................: 99.87% ✓ 8956       ✗ 12    
     data_received..................: 45 MB   250 kB/s
     data_sent......................: 1.8 MB  10 kB/s
     http_req_blocked...............: avg=15.23µs  min=2µs     med=8µs      max=2.8ms    p(90)=18µs    p(95)=28µs   
     http_req_connecting............: avg=4.12µs   min=0s      med=0s       max=1.9ms    p(90)=0s      p(95)=0s     
     http_req_duration..............: avg=634.23ms min=12.34ms med=456.78ms max=8.2s     p(90)=1.2s    p(95)=1.8s   
       { expected_response:true }...: avg=632.45ms min=12.34ms med=455.12ms max=8.2s     p(90)=1.19s   p(95)=1.79s  
     http_req_failed................: 0.13%   ✓ 12         ✗ 8944
     http_req_receiving.............: avg=234.56µs min=45µs    med=178µs    max=12ms     p(90)=412µs   p(95)=567µs  
     http_req_sending...............: avg=78.34µs  min=21µs    med=56µs     max=3.4ms    p(90)=134µs   p(95)=189µs  
     http_req_tls_handshaking.......: avg=0s       min=0s      med=0s       max=0s       p(90)=0s      p(95)=0s     
     http_req_waiting...............: avg=633.91ms min=12.21ms med=456.45ms max=8.2s     p(90)=1.2s    p(95)=1.8s   
     http_reqs......................: 8956    49.75/s
     iteration_duration.............: avg=834ms    min=212ms   med=656ms    max=8.4s     p(90)=1.4s    p(95)=2.0s   
     iterations.....................: 8956    49.75/s
     vus............................: 1       min=1        max=30  
     vus_max........................: 30      min=30       max=30  

Custom Metrics:
     cache_hits.....................: 42.34%  (3792 hits / 8956 total)
     errors.........................: 0.13%   (12 errors)
     latency_ms.....................: avg=456ms p(95)=1800ms p(99)=3200ms
```

## Analysis

### What Worked

1. **Throughput: 50 QPS**
   - Stable at ~50 queries per second
   - Way below target 1,000 QPS, but my laptop can only do so much
   - No crashes or data loss

2. **Cache Hit Rate: 42.34%**
   - Lower than target 80%
   - Issue: Test uses 4 different query patterns with random parameters
   - Real production traffic would have more repetition

3. **Cursor Pagination**
   - Works correctly, no duplicate or skipped results
   - Much faster than offset pagination (tested separately)
   - Scales well even with 10M rows

### What Didn't Work

1. **p95 Latency: 1.8s**
   - ❌ Misses target SLA of p95 < 1000ms
   - Uncached queries take 1-2 seconds
   - Database is the bottleneck

2. **p99 Latency: 3.2s**
   - ❌ Way over target
   - Some queries took 8+ seconds
   - These were the aggregation queries

3. **Cache Hit Rate: 42%**
   - ❌ Misses target 80%
   - Test pattern is too random
   - Need better test data distribution

4. **Redis Memory Exhaustion**
   - Around 2:15 mark, Redis hit 512MB limit
   - Started evicting cached queries
   - Cache hit rate dropped from 55% to 35%

### Bottlenecks Identified

1. **PostgreSQL Query Performance**
   - COUNT(*) queries take 500-1000ms on 10M rows
   - Even with indexes, full table scans are slow
   - Need query optimization or materialized views

2. **Redis Memory Limit**
   - 512MB is too small for production
   - Cached queries are large (50-100KB each)
   - Need 2-4GB for production

3. **Cursor Pagination with COUNT**
   - Cursor pagination is fast (50-100ms)
   - But COUNT(*) for totalCount is slow (500-1000ms)
   - Could skip COUNT for better performance

4. **JSON Serialization**
   - Serializing 50-100 events to JSON takes 20-50ms
   - Not a major bottleneck, but adds up

## Lessons Learned

1. **COUNT(*) is Expensive**
   - Initially included totalCount in every response
   - Realized it's killing performance
   - Made it optional - only compute when requested

2. **Redis Eviction Policy**
   - Started with allkeys-random eviction
   - Switched to allkeys-lru (least recently used)
   - Improved cache hit rate by 10%

3. **Cursor vs Offset**
   - Tested offset pagination for comparison
   - Offset 10000 took 2.5 seconds
   - Cursor took 80ms
   - Cursor is 30x faster

## Recommendations

1. **Remove COUNT from Default Response**
   - Make totalCount optional
   - Would improve p95 from 1.8s to ~500ms
   - Tradeoff: Less info for clients

2. **Increase Redis Memory**
   - 512MB is too small
   - Recommend 2GB for production
   - Would improve cache hit rate to 70-80%

3. **Add Materialized Views**
   - Pre-compute common aggregations
   - Refresh every 5 minutes
   - Would make aggregation queries instant

4. **Revise SLA Targets**
   - p95 < 1000ms is not achievable with COUNT
   - More realistic: p95 < 500ms (without COUNT), p95 < 2000ms (with COUNT)

## Conclusion

The system works, but performance is not great. The main issues are:
1. COUNT(*) queries are too slow
2. Redis memory is too small
3. Cache hit rate is lower than expected

The good news: cursor pagination works great and scales well. The architecture is sound, just need some tuning.

**Key Takeaway:** Analytics queries are fundamentally slow. You can't query 100B rows in real-time. Need to either pre-compute results (materialized views) or accept higher latency.
